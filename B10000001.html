<!DOCTYPE html>
<html lang="en" class="no-js" >
<head>

    <!--- basic page needs
    ================================================== -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Multi-level Caching - Jian Deng</title>

    <script>
        document.documentElement.classList.remove('no-js');
        document.documentElement.classList.add('js');
    </script>

    <!-- CSS
    ================================================== -->
    <link rel="stylesheet" href="css/vendor.css">
    <link rel="stylesheet" href="css/styles.css">

    <!-- favicons
    ================================================== -->
    <link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
    <link rel="manifest" href="site.webmanifest">

</head>

<body id="top">


<!-- preloader
================================================== -->
<div id="preloader">
    <div id="loader" class="dots-fade">
        <div></div>
        <div></div>
        <div></div>
    </div>
</div>


<!-- page wrap
================================================== -->
<div id="page" class="s-pagewrap">


    <!-- # site header
    ================================================== -->
    <header id="masthead" class="s-header">

        <div class="s-header__branding">
            <p class="site-title">
                <a href="index.html" rel="home">Jian Deng.</a>
            </p>
        </div>

        <div class="row s-header__navigation">

            <nav class="s-header__nav-wrap">

                <h3 class="s-header__nav-heading">Navigate to</h3>

                <ul class="s-header__nav">
                    <li><a href="index.html" title="">Home</a></li>
                    <li class="has-children">
                        <a href="#0" title="" class="">Categories</a>
                        <ul class="sub-menu">
                            <li><a href="category.html">Design</a></li>
                            <li><a href="category.html">Lifestyle</a></li>
                            <li><a href="category.html">Inspiration</a></li>
                            <li><a href="category.html">Work</a></li>
                            <li><a href="category.html">Health</a></li>
                            <li><a href="category.html">Photography</a></li>
                        </ul>
                    </li>
                    <li class="current-menu-item has-children">
                        <a href="#0" title="" class="">Blog</a>
                        <ul class="sub-menu">
                            <li><a href="single-standard.html">Standard Post</a></li>
                            <li><a href="single-video.html">Video Post</a></li>
                            <li><a href="single-audio.html">Audio Post</a></li>
                        </ul>
                    </li>
                    <li><a href="styles.html" title="">Styles</a></li>
                    <li><a href="about.html" title="">About</a></li>
                    <li><a href="contact.html" title="">Contact</a></li>
                </ul> <!-- end s-header__nav -->

            </nav> <!-- end s-header__nav-wrap -->

        </div> <!-- end s-header__navigation -->

        <div class="s-header__search">

            <div class="s-header__search-inner">
                <div class="row">

                    <form role="search" method="get" class="s-header__search-form" action="#">
                        <label>
                            <span class="u-screen-reader-text">Search for:</span>
                            <input type="search" class="s-header__search-field" placeholder="Search for..." value="" name="s" title="Search for:" autocomplete="off">
                        </label>
                        <input type="submit" class="s-header__search-submit" value="Search">
                    </form>

                    <a href="#0" title="Close Search" class="s-header__search-close">Close</a>

                </div> <!-- end row -->
            </div> <!-- s-header__search-inner -->

        </div> <!-- end s-header__search -->

        <a class="s-header__menu-toggle" href="#0"><span>Menu</span></a>
        <a class="s-header__search-trigger" href="#">
            <svg width="24" height="24" fill="none" viewBox="0 0 24 24">
                <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M19.25 19.25L15.5 15.5M4.75 11C4.75 7.54822 7.54822 4.75 11 4.75C14.4518 4.75 17.25 7.54822 17.25 11C17.25 14.4518 14.4518 17.25 11 17.25C7.54822 17.25 4.75 14.4518 4.75 11Z"></path>
            </svg>
        </a>

    </header> <!-- end s-header -->

    <div id="content" class="s-content s-content--blog">

        <div class="row entry-wrap">
            <div class="column lg-12">

                <article class="entry format-standard">

                    <header class="entry__header">

                        <h1 class="entry__title">
                            Multi-level Caching.
                        </h1>

                        <div class="entry__meta">
                            <div class="entry__meta-author">
                                <svg width="24" height="24" fill="none" viewBox="0 0 24 24">
                                    <circle cx="12" cy="8" r="3.25" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5"></circle>
                                    <path stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="1.5" d="M6.8475 19.25H17.1525C18.2944 19.25 19.174 18.2681 18.6408 17.2584C17.8563 15.7731 16.068 14 12 14C7.93201 14 6.14367 15.7731 5.35924 17.2584C4.82597 18.2681 5.70558 19.25 6.8475 19.25Z"></path>
                                </svg>
                                <a href="#">Jian Deng</a>
                            </div>
                            <div class="entry__meta-date">
                                <svg width="24" height="24" fill="none" viewBox="0 0 24 24">
                                    <circle cx="12" cy="12" r="7.25" stroke="currentColor" stroke-width="1.5"></circle>
                                    <path stroke="currentColor" stroke-width="1.5" d="M12 8V12L14 14"></path>
                                </svg>
                                August 15, 2023
                            </div>
                        </div>
                    </header>

                    <div class="content-primary">

                        <div class="entry__content">
                            <p>Cache technology is a well-discussed topic and a versatile tool for solving performance issues. It is often a subject of questions in various interviews, covering topics such as cache algorithms, hotspots data and cache updates, cache crashes, and rapid recovery. Some of these questions are context-specific, making the rational application of caching to solve problems a choice. This article primarily discusses caching solutions from the perspectives of architecture and improving cache hit rates. The article introduces issues and solutions related to application caching based on a multi-level caching model, some of which are already implemented, while others are experimental solutions.</p>

                            <h2>1. Introduction to Multi-level Caching</h2>

                            <p>Multi-level caching refers to caching data at different levels of the system architecture to improve access efficiency, making it one of the most widely used solutions. The overall architecture and process of our application are as follows:</p>

                            <ol>
                                <li>Nginx handles incoming requests and balances the load to application Nginx using commonly used load balancing algorithms such as round-robin or consistent hashing. Round-robin ensures a more balanced distribution of server requests, while consistent hashing enhances the cache hit rate for application Nginx.</li>
                                <li>Application Nginx reads from local caches (which can use Lua Shared Dict, Nginx Proxy Cache - disk/memory, or Local Redis implementation). If the local cache is hit, it directly returns the data, improving overall throughput and reducing backend pressure, particularly effective for addressing hot spot issues.</li>
                                <li>If the Nginx local cache is not hit, it reads the corresponding distributed cache (such as Redis cache, considering master-slave architecture to enhance performance and throughput). If the distributed cache is hit, it directly returns the corresponding data (and writes back to Nginx local cache).</li>
                                <li>If the distributed cache is also not hit, it falls back to the Tomcat cluster, and during this process, round-robin and consistent hashing can be used as load balancing algorithms.</li>
                                <li>In the Tomcat application, it first reads from the local heap cache. If present, it directly returns (and writes to the main Redis cluster).</li>
                                <li>Optionally, if step 4 does not hit, it can attempt another read from the main Redis cluster to prevent a traffic surge when the secondary cluster has issues.</li>
                                <li>If all caches fail to hit, it resorts to querying the database or relevant services to obtain the necessary data and returns it.</li>
                                <li>The data returned in step 7 is asynchronously written to the main Redis cluster, and here, multiple Tomcat instances may simultaneously write to the main Redis cluster, potentially causing data inconsistencies.</li>
                                <li>The overall caching is divided into three parts: application Nginx local cache, distributed cache, and Tomcat heap cache. Each layer of the cache is used to address specific issues, such as application Nginx local cache for hot spot caching issues, distributed cache to reduce access to the backend, and Tomcat heap cache to prevent impacts after related cache invalidation or crashes. Although all involve adding caches, the specific methods and considerations are diverse.</li>
                            </ol>

                            <h2>2. How to Cache Data</h2>

                            <h3>2.1 Expiry and Non-Expiry</h3>

                            <p>For cached data, we can consider both non-expiring and time-limited caches, depending on factors such as business needs and data volume.</p>

                            <p>Non-expiring process:</p>

                            <ol>
                                <li>Start a transaction.</li>
                                <li>Execute SQL and business logic.</li>
                                <li>Commit the transaction.</li>
                                <li>Write to the cache.</li>
                            </ol>

                            <p>Using the Cache-Aside pattern, data is first written to the database, and if successful, it is then written to the cache. In this scenario, there may be cases where the transaction is successful and cache write fails, but without the ability to roll back the transaction. It is advised not to include cache writes within transactions, especially for distributed caches, as unstable network may lead to slow cache response times, causing database transaction blocks. If data consistency for cache is not critical, and the data volume is not large, periodic full cache synchronization can be considered. To address issues related to multiple transactions, consider using the "Queue Technology".</p>

                            <p>For data with long-tail access, high-frequency access, or sufficient cache space, non-expiring caching can be considered. Examples include user data, categories, products, prices, orders, etc. When the cache is full, the LRU mechanism can be employed to exclude old cache data.</p>

                            <p>For expiry-based caching, lazy loading can be used and is generally applied to cache data from other systems (where change notifications are not available or have a high cost), scenarios with limited cache space, or low-frequency hotspots. The common steps involve first reading the cache; if a cache miss occurs, query the data, asynchronously write it to the cache, and set an expiration time. Hotspot data often uses expiry-based caching, meaning a short caching time on the application. This type of cache may have temporary data inconsistency but is acceptable depending on the scenario.</p>

                            <h3>2.2 Dimensional Caching and Incremental Caching</h3>

                            <p>For an advertising system, a single ad may have various attributes such as basic properties, image lists, availability, specifications, and ad descriptions. Updating all this data when an ad changes can be costly in terms of API calls and bandwidth. Therefore, it is beneficial to dimensionalize the data and perform incremental updates (updating only the changed parts). This approach is particularly useful for data that changes frequently, such as availability status changes.</p>

                            <h3>2.3 Large Value Caching</h3>

                            <p>Large values in the cache, especially when using Redis, can be a concern. In such cases, consider using a multi-threaded cache implementation like Memcached or compressing values. Another approach is to split the large value into smaller values, allowing the client to query and aggregate them.</p>

                            <h3>2.4 Hotspot Caching</h3>

                            <p>For highly accessed hotspots, fetching data from a remote cache system for each request can lead to issues such as excessive requests, high loads, and high bandwidth usage on the remote cache system, resulting in slow cache responses and client timeouts. One solution is to add more slave caches, and clients can read from these caches using a load balancing mechanism. Alternatively, in the application layer where the client is located, a local cache can be maintained to avoid accessing the remote cache. Even for data like inventory, some applications can use a short local cache, reducing pressure on the remote system.</p>

                            <h2>3. Distributed Cache and Application Load Balancing</h2>

                            <h3>3.1 Cache Distribution</h3>

                            <p>Distributed caching typically involves sharding, where data is distributed across multiple instances or servers. Algorithms commonly used are modulo and consistent hashing. For non-expiring cache mechanisms, modulo can be considered, and when scaling, a new cluster can be created. For caches that can be lost, consistent hashing can be considered to ensure that even if one instance is lost, only a small portion of the cache is affected. Inconsistent caches can be addressed by client-side implementation or using middleware such as Twemproxy for proxying (transparent to the client). Redis users may consider using the redis-cluster distributed cluster solution.</p>

                            <h3>3.2 Application Load Balancing</h3>

                            <p>Application load balancing typically uses round-robin and consistent hashing. Consistent hashing routes similar requests to the same node based on the URL or URL parameters. Round-robin evenly distributes requests to each server. The overall flow is as follows:</p>

                            <ol>
                                <li>Requests first enter the Nginx access layer.</li>
                                <li>Based on the load balancing algorithm, requests are forwarded to the application Nginx.</li>
                                <li>If the application Nginx local cache is hit, it directly returns the data; otherwise, it reads from the distributed cache or falls back to Tomcat.</li>
                            </ol>

                            <p>The advantages of round-robin include more even requests to the application Nginx, balancing the load on each server. The drawback is that as the number of servers increases, the cache hit rate may decrease. For example, with 10 servers, the hit rate may be 90%, but adding 10 more servers could reduce it to 45%. However, this method does not lead to a server being overloaded due to hotspot issues.</p>

                            <p>Consistent hashing ensures that similar requests are routed to the same server, maintaining hit rates even with server additions. The drawback is that it may cause a single server to be overloaded, potentially causing service issues.</p>

                            <p>To address these issues, the choice of algorithm can be dynamic:</p>

                            <ol>
                                <li>Use consistent hashing when the load is low.</li>
                                <li>Degrade consistent hashing to round-robin for hotspots or consider weighted consistent hashing if there is a pattern in request data.</li>
                                <li>Push hot data to the access layer Nginx for direct user response.</li>
                            </ol>

                            <h2>4. Hotspot Data and Cache Updates</h2>

                            <p>Hotspot data can lead to excessive server pressure, affecting server performance, throughput, and bandwidth, resulting in slow responses or service refusal, which is unacceptable. Several solutions can be considered:</p>

                            <h3>4.1 Single-Node Full Cache and Master-Slave</h3>

                            <p>In this approach, all caches are stored on the local storage, and after sourcing, the data is updated to the main Redis cluster, then replicated through the master-slave mode to other Redis clusters. Cache updates can be done lazily or synchronized through subscribed messages.</p>

                            <h3>4.2 Distributed Cache and Local Application Hotspot</h3>

                            <p>For distributed caching, application caching can be applied in the Nginx + Lua to reduce the impact on the Redis cluster. The process involves querying the local cache first; if hit, cache directly, if not, query the Redis cluster, fall back to Tomcat, and cache the data locally. For the LVS+HAProxy to the application Nginx load mechanism, consistent hashing is used in normal circumstances. Still, if the access volume of a certain request type exceeds a threshold, it automatically degrades to a round-robin mechanism. For hotspots like flash sales, related data can be pushed to the application Nginx in advance, and the load balancing mechanism can be downgraded to round-robin. In practical scenarios, this feature is implemented using a two-level Nginx (access Nginx → application Nginx), not at the LVS+HAProxy layer.</p>

                            <p>Additionally, consider establishing a real-time hotspot discovery system to identify hotspots.</p>

                            <p>The specific steps are as follows:</p>

                            <ol>
                                <li>Ingress Nginx forwards requests to application Nginx.</li>
                                <li>Application Nginx first reads from the local cache. If hit, it directly returns; if not, it reads from the distributed cache, falls back to Tomcat for processing.</li>
                                <li>Application Nginx reports the request to the real-time hotspot discovery system, either by directly reporting via UDP, writing requests to local Kafka, or subscribing to local Nginx logs using Flume. After reporting to the real-time hotspot discovery system, it conducts hotspot statistics (considering real-time computation with Storm, for example).</li>
                                <li>Push hot data to the application Nginx local cache based on the set threshold.</li>
                                <li>With local caching, data consistency must be considered, determining when to invalidate or update the cache:</li>
                                <ul>
                                    <li>If subscribing to data change messages is possible, it is recommended to subscribe to change messages for cache updates.</li>
                                    <li>If message subscription is not possible or is costly, and if the requirement for short-term data consistency is not strict, a reasonable expiration time can be set, and new data can be queried after expiration.</li>
                                    <li>For events like flash sales, subscribe to the activity start message, push relevant data to the frontend application in advance, and downgrade the load balancing mechanism to round-robin.</li>
                                    <li>Establish a real-time hotspot discovery system for unified hotspot push and updates.</li>
                                </ul>
                            </ol>

                            <h2>5. Cache Updates and Atomicity</h2>

                            <p>As mentioned earlier, if multiple applications simultaneously operate on the same data, it can lead to cache data becoming dirty. Solutions include:</p>

                            <ul>
                                <li>Use update timestamps or version comparison during data updates. For Redis users, leverage its single-threaded mechanism for atomic updates.</li>
                                <li>Subscribe to the database binlog using tools like Canal.</li>
                                <li>Distribute update requests to multiple queues based on specific rules, and then update each queue in a single-threaded manner, fetching the latest data during updates.</li>
                                <li>Use distributed locks to obtain relevant locks before updating.</li>
                            </ul>

                            <h2>6. Cache Crashes and Rapid Recovery</h2>

                            <h3>6.1 Modulo</h3>

                            <p>For the modulo mechanism, if one instance fails, removing that instance may result in a large number of cache misses. Instantaneous high traffic may cause issues with the backend database or services. To address this issue, a master-slave mechanism can be used to avoid problems when one instance fails, allowing another instance to take its place. However, adding a node to the modulo mechanism can lead to a large number of cache misses. Typically, a new cluster is established, and data is migrated to the new cluster, directing traffic to the new cluster.</p>

                            <h3>6.2 Consistent Hashing</h3>

                            <p>For consistent hashing, if one instance fails, removing that instance only affects a portion of the cache on the consistent hashing ring, preventing a large number of cache misses and traffic redirection to the backend database or services. However, there may still be some impact. Additionally, the entire cache cluster may experience issues due to accidental operations. How to recovery rapidly?</p>

                            <h3>6.3 Fast Recovery</h3>

                            <p>If issues arise as mentioned earlier, consider the following solutions:</p>
                            <ol>
                                <li>Implement a master-slave mechanism with proper redundancy, i.e., if a part becomes unavailable, compensate by bringing up the corresponding part.</li>
                                <li>If the application's availability has decreased due to caching issues, consider degrading some users and gradually reducing the degradation level. Use workers in the background to preheat cache data.</li>
                                <li>In other words, if the entire cache cluster fails and there is no backup, the only option is to slowly rebuild the cache. To keep some users available, employ a degradation plan based on the system's capacity, allowing a portion of users to be functional while rebuilding the cache related to these users. Additionally, perform cache data preheating through background workers.</li>
                            </ol>
                        </div>
                    </div>

                </article> <!-- end entry -->

            </div>
        </div> <!-- end entry-wrap -->
    </div> <!-- end s-content -->


    <!-- Java Script
    ================================================== -->
    <script src="js/plugins.js"></script>
    <script src="js/main.js"></script>
</div>
</body>
</html>